{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from vrnn import utils\n",
    "from vrnn.models import VanillaModule\n",
    "from vrnn.normalization import NormalizedDataset, NormalizationModule\n",
    "from vrnn.data_thermal import DatasetThermal, VoigtReussThermNormalization\n",
    "import numpy as np\n",
    "from vrnn.tensortools import unpack_sym\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "dtype = torch.float32\n",
    "\n",
    "data_dir = utils.get_data_dir()\n",
    "\n",
    "import shutil\n",
    "import matplotlib\n",
    "plt.rcParams[\"text.usetex\"] = True if shutil.which('latex') else False\n",
    "matplotlib.rcParams[\"text.latex.preamble\"] = r\"\\usepackage{amsmath}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c39d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VRNN model\n",
    "model_norm_file = data_dir / 'Thermal2D_models/alpha/vrnn_therm2D_norm_20250121_173200.pt'\n",
    "# Vanilla model\n",
    "model_vanilla_file = data_dir / 'Thermal2D_models/alpha/vann_therm2D_20250121_202555.pt'\n",
    "\n",
    "# Load hdf5 files\n",
    "ms_file = '/media/ssd/keshav/feature_engineering_thermal_2D.h5'\n",
    "print(ms_file)\n",
    "\n",
    "# Load data\n",
    "feature_idx = None\n",
    "R_range_train = [1/100., 1/50., 1/20., 1/10., 1/5., 1/2., 2, 5, 10, 20, 50, 100]\n",
    "train_data = DatasetThermal(file_name=ms_file, R_range=R_range_train, group='train_set',\n",
    "                            input_mode='descriptors', feature_idx=feature_idx, feature_key='feature_vector', ndim=2)\n",
    "\n",
    "\n",
    "R_range_val = np.concatenate([np.arange(2, 101, dtype=int), 1. / np.arange(1, 101, dtype=int)])\n",
    "val_data = DatasetThermal(file_name=ms_file, R_range=R_range_val, group='val_set',\n",
    "                          input_mode='descriptors', feature_idx=feature_idx, feature_key='feature_vector', ndim=2)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 30000\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Fetch data\n",
    "train_x, train_y = utils.get_data(train_loader, device=device, dtype=dtype)\n",
    "val_x, val_y = utils.get_data(val_loader, device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "# Define normalization\n",
    "features_max = torch.cat([train_x, val_x], dim=0).max(dim=0)[0]\n",
    "features_min = torch.cat([train_x, val_x], dim=0).min(dim=0)[0]\n",
    "features_min[0],features_max[0]  = 0, 1 # Dont normalize the first feature (volume fraction)\n",
    "# features_min, features_max = None, None\n",
    "normalization = VoigtReussThermNormalization(dim=2, features_min=features_min, features_max=features_max)\n",
    "\n",
    "# Normalize data\n",
    "train_data_norm = NormalizedDataset(train_data, normalization)\n",
    "val_data_norm = NormalizedDataset(val_data, normalization)\n",
    "train_loader_norm = DataLoader(train_data_norm, batch_size=batch_size, shuffle=False)\n",
    "val_loader_norm = DataLoader(val_data_norm, batch_size=batch_size, shuffle=False)\n",
    "train_x_norm, train_y_norm = utils.get_data(train_loader_norm, device=device, dtype=dtype)\n",
    "val_x_norm, val_y_norm = utils.get_data(val_loader_norm, device=device, dtype=dtype)\n",
    "\n",
    "# VRNN normalized model\n",
    "\n",
    "ann_model = torch.load(model_norm_file, map_location=device, weights_only=False).to(device=device, dtype=dtype)\n",
    "model_norm = VanillaModule(ann_model).to(device=device, dtype=dtype)\n",
    "model_norm.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    train_pred_norm = model_norm(train_x_norm)\n",
    "    val_pred_norm = model_norm(val_x_norm)\n",
    "\n",
    "# VRNN model\n",
    "\n",
    "model = NormalizationModule(normalized_module=model_norm, normalization=normalization).to(device=device, dtype=dtype)\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    train_pred = model(train_x)\n",
    "    val_pred = model(val_x)\n",
    "\n",
    "# Vanilla model\n",
    "ann_model = torch.load(model_vanilla_file, map_location=device, weights_only=False).to(device=device, dtype=dtype)\n",
    "model_vanilla = VanillaModule(ann_model).to(device=device, dtype=dtype)\n",
    "model_vanilla.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    train_pred_vanilla = model_vanilla(train_x)\n",
    "    val_pred_vanilla = model_vanilla(val_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e50c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1.  normalisation wrappers \n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def encode_x(x_raw):\n",
    "    \"\"\"\n",
    "    raw (53,)  ➜  x̄ (53,)\n",
    "    \"\"\"\n",
    "    return normalization.normalize_x(x_raw.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "def decode_x(x_bar):\n",
    "    \"\"\"\n",
    "    x̄ (53,)  ➜  raw (53,)  (physical units)\n",
    "    \"\"\"\n",
    "    return normalization.reconstruct_x(x_bar.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "def decode_y(x_raw, y_norm):\n",
    "    \"\"\"\n",
    "    (x_raw, ȳ) ➜ y_raw  (physical units)\n",
    "    Still fully differentiable.\n",
    "    \"\"\"\n",
    "    return normalization.reconstruct(x_raw.unsqueeze(0), y_norm.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2.  forward pass\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def forward_raw(x_raw, use_norm=True):\n",
    "    \"\"\"\n",
    "    x_raw (53,) ─► κ_raw  (κ11, κ22, κ12)\n",
    "    \"\"\"\n",
    "    x_raw = x_raw.to(device)\n",
    "    if use_norm:\n",
    "        y_raw = model(x_raw.unsqueeze(0)).squeeze(0) \n",
    "    else:\n",
    "        y_raw = model_vanilla(x_raw.unsqueeze(0)).squeeze(0)\n",
    "    return y_raw\n",
    "\n",
    "def raw_forward_from_xbar(x_bar):\n",
    "    \"\"\"\n",
    "    x_bar  (53,) in [0,1]  --->  κ_raw (κ11, κ22, κ12)  physical units\n",
    "    • keeps gradients (no torch.no_grad)\n",
    "    • uses model_norm (x̄ ➜ ȳ)\n",
    "    \"\"\"\n",
    "    y_bar = model_norm(x_bar.unsqueeze(0)).squeeze(0)           # ȳ\n",
    "    x_raw = normalization.reconstruct_x(x_bar.unsqueeze(0)).squeeze(0)  # raw x\n",
    "    y_raw = decode_y(x_raw, y_bar)  # κ_raw\n",
    "    return y_raw, y_bar\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3.  utility metrics\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def anisotropy(k):\n",
    "    \"\"\"\n",
    "    Compute the anisotropy defined as λ_max / λ_min, where λ_min is clamped to eps\n",
    "    for numerical stability. Works for both a single sample (1D tensor) and batches (2D tensor).\n",
    "    \"\"\"\n",
    "    eps = 1e-8\n",
    "    unsqueezed = False\n",
    "    # If a single sample is provided, add a batch dimension.\n",
    "    if k.dim() == 1:\n",
    "        k = k.unsqueeze(0)\n",
    "        unsqueezed = True\n",
    "    # Convert from Voigt notation to symmetric matrices.\n",
    "    K = unpack_sym(k, dim=2)  # Expected shape: (B, 2, 2)\n",
    "    # Compute eigenvalues for each symmetric matrix.\n",
    "    eigvals = torch.linalg.eigvalsh(K)  # Shape: (B, 2), sorted in ascending order.\n",
    "    # Compute anisotropy: λ_max / max(λ_min, eps) ensuring no div-by-zero.\n",
    "    ratio = eigvals[:, 1] / torch.clamp(eigvals[:, 0], min=eps)\n",
    "    return ratio[0] if unsqueezed else ratio\n",
    "\n",
    "def kbar(k):                          # mean of κ11, κ22\n",
    "    return 0.5 * (k[0] + k[1])\n",
    "\n",
    "def random_raw_sample():\n",
    "    x_bar = torch.rand(53, device=device, dtype=dtype)\n",
    "    x = normalization.reconstruct_x(x_bar.unsqueeze(0)).squeeze(0)  # raw\n",
    "    return x, x_bar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# A.  maximise anisotropy at fixed R \n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def design_most_anisotropic(R,                        \n",
    "                            restarts=2,\n",
    "                            lr=1e-1,\n",
    "                            steps=200,\n",
    "                            use_norm=True):\n",
    "    best_val, best_x, best_k = -float('inf'), None, None\n",
    "    log_data_A = []\n",
    "\n",
    "    for restart in range(restarts):\n",
    "        # Initialize logging for this restart.\n",
    "        log_data_A.append({\"restart\": restart + 1, \"steps\": []})\n",
    "\n",
    "        x, _ = random_raw_sample()  # random raw sample (start somehwere inside the training data)\n",
    "        x[-2:] = torch.tensor([1.0 / R, R], device=device, dtype=dtype)  # enforce (1/R, R)\n",
    "        # x[0] = 0.001\n",
    "        x_raw = x\n",
    "        x_raw.requires_grad_(True)\n",
    "        opt = torch.optim.Adam([x_raw], lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=steps // 10, gamma=0.5)\n",
    "\n",
    "        for step in range(steps):\n",
    "            opt.zero_grad()\n",
    "            k_raw = forward_raw(x_raw, use_norm=use_norm) \n",
    "            \n",
    "            loss = -anisotropy(k_raw)\n",
    "            current_lr = opt.param_groups[0]['lr']\n",
    "            \n",
    "            # ----- Log the current step's data -----\n",
    "            log_entry = {\n",
    "                \"restart\": restart + 1,\n",
    "                \"step\": step + 1,\n",
    "                \"loss\": loss.item(),\n",
    "                \"anisotropy\": -loss.item(),  # note: loss was defined as -anisotropy\n",
    "                \"lr\": current_lr,\n",
    "                \"x_raw\": x_raw.detach().cpu().numpy(),\n",
    "                \"k_raw\": k_raw.detach().cpu().numpy()\n",
    "            }\n",
    "            log_data_A[restart][\"steps\"].append(log_entry)\n",
    "            print(f\"Restart {restart}/{restarts} - Step {step}/{steps}: anisotropy = {-loss.item():.4f}, lr = {current_lr:.6f}\")\n",
    "            # ---------------------------------------\n",
    "                                \n",
    "            loss.backward()\n",
    "            x_raw.grad[-2:] = 0  # do not backprop through the last two features (1/R, R)\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Clamp to [0,1] (volume fraction) *without* breaking the graph:\n",
    "            with torch.no_grad():\n",
    "                x_raw[0].clamp_(0.0, 1.0)\n",
    "\n",
    "        val = -loss.item()\n",
    "        if val > best_val:\n",
    "            best_val, best_x, best_k = val, x_raw.detach().clone(), k_raw.detach()\n",
    "\n",
    "    return (best_x,  # 53-D (raw)\n",
    "            best_k,  # κ_raw\n",
    "            best_val,  # anisotropy\n",
    "            log_data_A)  # log data for analysis\n",
    "\n",
    "# contrastR_A = 1/100\n",
    "# bx, bk, a, log_data_A  = design_most_anisotropic(R=contrastR_A, restarts=2, lr=1e-1, steps=200, use_norm=True)\n",
    "# print(\"anisotropy =\", a, \"\\nκ =\", bk.cpu().numpy())\n",
    "# print(\"x =\", bx.cpu().numpy())\n",
    "\n",
    "# best_dataset, best_sample, best_counterpart, best_idx = find_closest_sample(bk, [train_data, val_data], compare='targets')\n",
    "# print(f\"Closest sample (idx {best_idx}) in dataset {best_dataset}:\")\n",
    "# print(f\"x: {best_sample.cpu().numpy()}\")\n",
    "# print(f\"y: {best_counterpart.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44413ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global settings\n",
    "fontsize = 8\n",
    "plt.rcParams.update({'font.size': fontsize})\n",
    "plt.style.use('seaborn-v0_8-paper')  # Clean, publication-style theme\n",
    "\n",
    "def plot_predictions_v2(ax, x, y, pred, phase_contrast, is_last_row=False):\n",
    "    # Create mask for the specified phase contrast\n",
    "    mask = x[:, -1] == phase_contrast\n",
    "    x_filtered = x[mask]\n",
    "    y_filtered = y[mask]\n",
    "    pred_filtered = pred[mask]\n",
    "    \n",
    "    # Process data for plotting\n",
    "    volume_fraction = x_filtered[:, 0].cpu().numpy()\n",
    "    y_matrix = torch.stack([torch.stack([y_filtered[:, 0], y_filtered[:, 2]], dim=-1),\n",
    "                          torch.stack([y_filtered[:, 2], y_filtered[:, 1]], dim=-1)], dim=-2)\n",
    "    pred_matrix = torch.stack([torch.stack([pred_filtered[:, 0], pred_filtered[:, 2]], dim=-1),\n",
    "                           torch.stack([pred_filtered[:, 2], pred_filtered[:, 1]], dim=-1)], dim=-2)\n",
    "    \n",
    "    # Compute eigenvalues\n",
    "    y_eigenvalues = torch.linalg.eigvals(y_matrix).real.cpu().numpy()\n",
    "    pred_eigenvalues = torch.linalg.eigvals(pred_matrix).real.cpu().numpy()\n",
    "    \n",
    "    # Generate bounds\n",
    "    vf = np.linspace(0, 1, 1500)\n",
    "    voigt_bound = vf - (vf - 1) / phase_contrast\n",
    "    reuss_bound = 1 / (phase_contrast + vf - phase_contrast * vf)\n",
    "    \n",
    "    # Plot data\n",
    "    vf_repeated = np.repeat(volume_fraction, 2)\n",
    "    stacked_y = y_eigenvalues.reshape(-1)\n",
    "    stacked_pred = pred_eigenvalues.reshape(-1)\n",
    "    \n",
    "    ax.fill_between(vf, np.flip(voigt_bound), y2=np.flip(reuss_bound), alpha=0.2, color='tab:blue')\n",
    "    ax.plot(vf, np.flip(voigt_bound), 'k--', linewidth=0.5, alpha=0.3, label='Voigt-Reuss bounds' if is_last_row else None)\n",
    "    ax.plot( vf, np.flip(reuss_bound), 'k--', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # compute the voigt and reuss bound for each entry of vf_repeated\n",
    "    voigt_bound = (1-vf_repeated) - ((1-vf_repeated) - 1) / phase_contrast\n",
    "    reuss_bound = 1 / (phase_contrast + (1-vf_repeated) - phase_contrast * (1-vf_repeated))\n",
    "    \n",
    "    \n",
    "    ax.scatter(vf_repeated, stacked_y, s=1.5, color='limegreen',\n",
    "                edgecolor='black', linewidth=0.05, alpha=0.2)\n",
    "    ax.scatter([], [], s=3.0, color='limegreen', edgecolor='black', linewidth=0.05, alpha=1.0,\n",
    "                label=r'training data $\\lambda({\\overline{\\underline{\\underline{\\kappa}}}})$')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # violation_mask = (stacked_pred > voigt_bound) | (stacked_pred < reuss_bound)\n",
    "        \n",
    "    # ax.scatter(vf_repeated[~violation_mask], stacked_pred[~violation_mask], s=1.5, color='limegreen',\n",
    "    #             edgecolor='black', linewidth=0.05, alpha=0.2)\n",
    "    # ax.scatter([], [], s=3.0, color='limegreen', edgecolor='black', linewidth=0.05, alpha=1.0,\n",
    "    #            label=r'predicted $\\lambda({\\overline{\\underline{\\underline{\\kappa}}}})$' if is_last_row else None)\n",
    "    \n",
    "    \n",
    "    # # Violations!\n",
    "    # ax.scatter(vf_repeated[violation_mask], stacked_pred[violation_mask], s=3, color='red',\n",
    "    #            edgecolor='black', linewidth=0.05, alpha=1.0,\n",
    "    #           label=r'predicted $\\lambda({\\overline{\\underline{\\underline{\\kappa}}}})$ violation' if is_last_row else None)\n",
    "\n",
    "def plot_optimization_trajectories(ax, log_data, bx, bk):\n",
    "    for restart in log_data:\n",
    "        steps = [entry[\"step\"] for entry in restart[\"steps\"]]\n",
    "        x_raw = [entry[\"x_raw\"] for entry in restart[\"steps\"]]\n",
    "        vf = np.array([entry[\"x_raw\"][0] for entry in restart[\"steps\"]])\n",
    "        \n",
    "        k_raw = torch.tensor([entry[\"k_raw\"] for entry in restart[\"steps\"]])\n",
    "        k_raw = unpack_sym(k_raw, dim=2)\n",
    "        k_raw_eig = torch.linalg.eigvalsh(k_raw)  \n",
    "        k_raw_eig = k_raw_eig.detach().cpu().numpy()\n",
    "        \n",
    "        anisotropies = [entry[\"loss\"] for entry in restart[\"steps\"]]\n",
    "        \n",
    "        # Plot eigenvalues vs volume fraction (vf) with a different marker (e.g. square 's')\n",
    "        ax.plot(vf, k_raw_eig[:, 0], marker='o', linestyle='--', color='red', linewidth=0.5, markersize=3,\n",
    "                label=r\"optimization trajectory $\\widehat{\\lambda}_1$\" if restart[\"restart\"]==1 else \"\")\n",
    "        ax.plot(vf, k_raw_eig[:, 1], marker='o', linestyle='--', color='blue', linewidth=0.5, markersize=3,\n",
    "                label=r\"optimization trajectory $\\widehat{\\lambda}_2$\" if restart[\"restart\"]==1 else \"\")\n",
    "        \n",
    "        # scatter the best point\n",
    "        bk_mat = unpack_sym(torch.tensor(bk, device=device, dtype=dtype), dim=2)\n",
    "        bk_eig = torch.linalg.eigvalsh(bk_mat)\n",
    "        ax.scatter(bx[0].item(), bk_eig[0].item(), s=30, marker='D', color='lightgrey', edgecolor='black', linewidth=1.5, alpha=1.0,\n",
    "                label=r\"optimized candidate $\\lambda(\\widehat{\\overline{\\underline{\\underline{\\kappa}}}})$\" if restart[\"restart\"]==1 else None, zorder=10)\n",
    "        ax.scatter(bx[0].item(), bk_eig[1].item(), s=30, marker='D', color='lightgrey', edgecolor='black', linewidth=1.5, alpha=1.0,\n",
    "                label=r\"optimized candidate $\\widehat{\\lambda}_2$\" if restart[\"restart\"]==100 else None, zorder=10)\n",
    "        \n",
    "    ax.set_xlabel(fr'volume fraction $[-]$', fontsize=fontsize)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.set_box_aspect(1.0)\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.minorticks_on() # Add minor ticks\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    \n",
    "\n",
    "contrastR_A = 1/100\n",
    "bx_vrnn, bk_vrnn, _, log_data_A_vrnn = design_most_anisotropic(R=contrastR_A, restarts=4, lr=1e-1, steps=200, use_norm=True)\n",
    "bx_vann, bk_vann, _, log_data_A_vann = design_most_anisotropic(R=contrastR_A, restarts=4, lr=1e-1, steps=200, use_norm=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6.3, 3.5), dpi=300)\n",
    "plot_predictions_v2(ax[0], train_x, train_y, train_pred, contrastR_A, is_last_row=False)\n",
    "plot_optimization_trajectories(ax[0], log_data_A_vrnn, bx_vrnn, bk_vrnn)\n",
    "\n",
    "plot_predictions_v2(ax[1], train_x, train_y, train_pred_vanilla, contrastR_A, is_last_row=False)\n",
    "plot_optimization_trajectories(ax[1], log_data_A_vann, bx_vann, bk_vann)\n",
    "\n",
    "if contrastR_A < 1:\n",
    "    ax[0].set_title(f\"$\\\\text{{Voigt-Reuss net -}}\\\\, R = 1/{int(1/contrastR_A)}$\")\n",
    "    ax[1].set_title(f\"$\\\\text{{Vanilla NN -}}\\\\, R = 1/{int(1/contrastR_A)}$\")\n",
    "else:\n",
    "    ax[0].set_title(r'$R = ' + str(contrastR_A) + r'$')\n",
    "        \n",
    "handles, labels = ax[1].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.05), \n",
    "           ncol=4, fontsize=fontsize)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.15)  # Make room for legend\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(\"../../overleaf/gfx/therm2d_inverse_design_ex_A.png\", \n",
    "            bbox_inches='tight', \n",
    "            dpi=600,\n",
    "            metadata={'Creator': '', 'Producer': ''})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec3fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_sample(x, datasets, compare='features'):\n",
    "    \"\"\"\n",
    "    Find the closest sample among the provided datasets to the given point x.\n",
    "    Args:\n",
    "        x (torch.Tensor): reference sample.\n",
    "        datasets (list): list of dataset objects. Each dataset is expected to have both\n",
    "                         'features' and 'targets' attributes.\n",
    "        compare (str): attribute to compare - either 'features' or 'targets'.\n",
    "    Returns:\n",
    "        best_dataset: the dataset where the closest sample was found,\n",
    "        best_sample: the sample value from the chosen attribute,\n",
    "        best_counterpart: the sample’s counterpart (if compare is 'features', this is 'targets',\n",
    "                          and vice versa),\n",
    "        best_idx (int): index of the best sample within its dataset.\n",
    "    \"\"\"\n",
    "    x = x.to(device)\n",
    "    best_distance = float('inf')\n",
    "    best_dataset = None\n",
    "    best_sample = None\n",
    "    best_counterpart = None\n",
    "    best_idx = None\n",
    "\n",
    "    for dataset in datasets:\n",
    "        # Select the attribute for comparison.\n",
    "        comp_attr = getattr(dataset, compare).to(device)\n",
    "        if compare=='targets':\n",
    "            # comp_attr_sym = unpack_sym(comp_attr, dim=2)\n",
    "            # x_reshaped = unpack_sym(x.unsqueeze(0), dim=2)\n",
    "            # distances = torch.norm(comp_attr_sym - x_reshaped, p='fro', dim=(1,2))\n",
    "            \n",
    "            comp_attr_anisotropy = anisotropy(comp_attr)\n",
    "            x_anisotropy = anisotropy(x.unsqueeze(0))\n",
    "            distances = torch.abs(comp_attr_anisotropy - x_anisotropy)\n",
    "            \n",
    "        else:\n",
    "            if compare == 'normalized_x' or compare == 'features':\n",
    "                # penalize the first feature (volume fraction) to compute the distance\n",
    "                distances = torch.norm(comp_attr - x, dim=1) + 10*(comp_attr[:, 0] - x[0])**2\n",
    "            else:                \n",
    "                distances = torch.norm(comp_attr - x, dim=1)\n",
    "            \n",
    "        idx = torch.argmin(distances)\n",
    "        dist = distances[idx].item()\n",
    "\n",
    "        if dist < best_distance:\n",
    "            best_distance = dist\n",
    "            best_dataset = dataset\n",
    "            best_sample = comp_attr[idx]\n",
    "            # Get the other attribute as a counterpart.\n",
    "            if compare == 'features' or compare == 'targets':\n",
    "                other_attr = 'targets' if compare == 'features' else 'features'\n",
    "            elif compare == 'normalized_x' or compare == 'normalized_y':\n",
    "                other_attr = 'normalized_y' if compare == 'normalized_x' else 'normalized_x'\n",
    "            best_counterpart = getattr(dataset, other_attr)[idx]\n",
    "            best_idx = idx.item()\n",
    "\n",
    "    return best_dataset, best_sample, best_counterpart, best_idx\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "\n",
    "class MicrostructureImageDataset(Dataset):\n",
    "    def __init__(self, file_path, dataset_name):\n",
    "        self.file = h5py.File(file_path, 'r')\n",
    "        self.data = self.file[dataset_name]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        return torch.tensor(img).float()\n",
    "    \n",
    "# Load the dataset\n",
    "MS = MicrostructureImageDataset(file_path=ms_file, dataset_name='train_set/image_data')\n",
    "MS_bench = MicrostructureImageDataset(file_path=ms_file, dataset_name='benchmark_set/image_data')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Global settings\n",
    "fontsize = 8\n",
    "plt.rcParams.update({'font.size': fontsize})\n",
    "plt.style.use('seaborn-v0_8-paper')  # Clean, publication-style theme\n",
    "plt.rcParams[\"text.latex.preamble\"] = r\"\\usepackage{amssymb}\"\n",
    "\n",
    "N_TILES = 6\n",
    "FIGSIZE = (6.5, 3.5)\n",
    "\n",
    "log = log_data_A_vrnn[1][\"steps\"]\n",
    "\n",
    "images, titles = [], []\n",
    "c1_values = []\n",
    "unique_idxs = set()\n",
    "i = 0\n",
    "# Collect unique images until we have N_TILES (or run out of log entries)\n",
    "# while len(images) < N_TILES and i < len(log):\n",
    "for i in [0,1,2,3,4,49]:\n",
    "    entry = log[i]\n",
    "    \n",
    "    # # Comparing targets\n",
    "    # k_raw = torch.tensor(entry[\"k_raw\"], device=device)\n",
    "    # _, best_k_raw, _, best_idx = find_closest_sample(k_raw, [train_data], compare='targets')\n",
    "    \n",
    "    \n",
    "    # Comparing normalized features\n",
    "    x_raw = torch.tensor(entry[\"x_raw\"], device=device, dtype=dtype)\n",
    "    x_bar = encode_x(x_raw)\n",
    "    _, best_x_bar, best_k_bar, best_idx = find_closest_sample(x_bar, [train_data_norm], compare='normalized_x')\n",
    "    best_x = decode_x(best_x_bar)  # Get the raw counterpart of the best sample\n",
    "    best_k_raw = decode_y(best_x.to(device), best_k_bar.to(device))  # Get the raw counterpart of the best sample\n",
    "    \n",
    "    print(best_idx, best_k_raw.detach(), anisotropy(best_k_raw), entry['anisotropy'])\n",
    "    if best_idx is not None and best_idx not in unique_idxs:\n",
    "    # if True:\n",
    "        unique_idxs.add(best_idx)\n",
    "        img = MS[best_idx % 30000]  # MS dataset has 30000 samples\n",
    "        images.append(img.squeeze().cpu().numpy())\n",
    "        titles.append(\n",
    "            f\"Opt step - ${entry['step']}$\\n\"\n",
    "            f\"$\\\\mathcal{{J}}_\\\\mathrm{{opt}} = {entry['anisotropy']:.2f}$\\n\"\n",
    "            f\"$\\\\mathcal{{J}}_\\\\mathrm{{closest}} = {anisotropy(best_k_raw):.2f} \\\\Lsh$\"\n",
    "        )\n",
    "        # Store volume fraction to display under the image\n",
    "        c1_values.append(f\"$c_1 = {1-best_x[0].item():.2f}$\")\n",
    "    i += 1\n",
    "\n",
    "# Adjust the grid: if fewer images are found, use a balanced grid.\n",
    "rows = 1\n",
    "cols = math.ceil(len(images) / rows)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=FIGSIZE, squeeze=False, dpi=300)\n",
    "\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    if idx < len(images):\n",
    "        ax.imshow(images[idx], cmap='plasma')\n",
    "        ax.set_title(titles[idx], fontsize=8)\n",
    "        \n",
    "        # Add volume fraction text below the image\n",
    "        ax.text(0.5, -0.05, c1_values[idx], transform=ax.transAxes, \n",
    "                ha='center', va='top', fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(\"../../overleaf/gfx/therm2d_inverse_design_microstructures.png\", \n",
    "            bbox_inches='tight', \n",
    "            dpi=600,\n",
    "            metadata={'Creator': '', 'Producer': ''})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d15f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, log_data_A_vrnn_all = design_most_anisotropic(R=contrastR_A, restarts=100, lr=1e-1, steps=50, use_norm=True)\n",
    "\n",
    "\n",
    "# Plot all optimization trajectories x- steps y- anisotropy of all restarts and draw the best one boldly and all the others faintly\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global settings\n",
    "fontsize = 8\n",
    "plt.rcParams.update({'font.size': fontsize})\n",
    "plt.style.use('seaborn-v0_8-paper')  # Clean, publication-style theme\n",
    "plt.rcParams[\"text.latex.preamble\"] = r\"\\usepackage{amssymb}\"\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3.0, 1.5), dpi=300)\n",
    "\n",
    "# Identify best trajectory based on the final anisotropy value (largest)\n",
    "best_final = -float('inf')\n",
    "best_traj = None\n",
    "for traj in log_data_A_vrnn_all:\n",
    "    anisotropies = [step[\"anisotropy\"] for step in traj[\"steps\"]]\n",
    "    if anisotropies[-1] > best_final:\n",
    "        best_final = anisotropies[-1]\n",
    "        best_traj = traj\n",
    "\n",
    "# Plot each trajectory; best one in bold blue, others in faint gray\n",
    "for traj in log_data_A_vrnn_all:\n",
    "    steps = [step[\"step\"]-1 for step in traj[\"steps\"]]\n",
    "    anisotropies = [step[\"anisotropy\"] for step in traj[\"steps\"]]\n",
    "    if traj is best_traj:\n",
    "        ax.plot(steps, anisotropies, '-X',color=\"tab:blue\", linewidth=1, markersize=2.5, label=\"Best trajectory\", zorder=10)\n",
    "    else:\n",
    "        ax.plot(steps, anisotropies, color=\"gray\", linewidth=0.25, alpha=0.25)\n",
    "    \n",
    "    # Highlight theoretical anisotropy max which is (R+1)^2/(4*R) where R=1/100\n",
    "    theoretical_max = (contrastR_A + 1)**2 / (4 * contrastR_A)\n",
    "    ax.axhline(theoretical_max, color='red', linestyle='--', linewidth=0.5, alpha=0.5,\n",
    "               label=r'$\\mathcal{J}_\\mathrm{max}$' if traj is best_traj else None)\n",
    "\n",
    "ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_xlim(0, 50)\n",
    "ax.set_ylim(0, 1.05 * theoretical_max)\n",
    "ax.minorticks_on() # Add minor ticks\n",
    "ax.set_xlabel(\"optimization steps $[-]$\", fontsize=fontsize)\n",
    "ax.set_ylabel(\"Anisotropy $[-]$\", fontsize=fontsize)\n",
    "# ax.set_yscale('log')\n",
    "ax.legend(loc='lower right', fontsize=fontsize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502dbd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# B. design least phase contrast given target κ\n",
    "# TODO: make it work for both normalized and vanilla models\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "def design_least_phase_contrast_given_k(k_target,\n",
    "                                        restarts=3,\n",
    "                                        lr=1e-2,\n",
    "                                        steps=500,\n",
    "                                        beta=0.1):\n",
    "    \"\"\"\n",
    "    Given a target κ tensor k_target, optimizes for a raw sample x such that:\n",
    "      - The predicted κ = model(x) is close to k_target (MSE loss)\n",
    "      - The phase contrast R (last feature) is minimized (weighted by beta)\n",
    "    In the constructed x:\n",
    "      - x[0] (volume fraction) and R are clamped between 0 and 1,\n",
    "      - The last but one feature is forced to 1/R.\n",
    "    Returns:\n",
    "      best_x: optimized raw sample (53-dimensional)\n",
    "      best_k: model(x) for that sample,\n",
    "      best_R: optimized R value,\n",
    "      best_loss: final loss value\n",
    "    \"\"\"\n",
    "    best_loss = float('inf')\n",
    "    best_x = None\n",
    "    best_k = None\n",
    "    best_R_val = None\n",
    "\n",
    "    for r in range(restarts):\n",
    "        # start from a random raw sample and get its normalized version\n",
    "        x_rand, _ = random_raw_sample()  # random raw sample\n",
    "        x_bar = encode_x(x_rand).detach().clone()\n",
    "        x_bar.requires_grad_(True)\n",
    "        # Initialize R as a parameter; we start from 0.5 (in [0,1])\n",
    "        R_param = torch.tensor(0.5, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "        opt = torch.optim.Adam([x_bar, R_param], lr=lr)\n",
    "\n",
    "        for i in range(steps):\n",
    "            opt.zero_grad()\n",
    "            # Decode x from x_bar\n",
    "            x_dec = decode_x(x_bar)\n",
    "            # Override the last two features: set feature[-2] = 1/R and feature[-1] = R.\n",
    "            # (Safe division: R_param is clamped away from 0.)\n",
    "            x_mod = torch.cat([x_dec[:-2], torch.stack([1.0/R_param, R_param])])\n",
    "            # Get the predicted κ from the raw model input\n",
    "            k_pred = model(x_mod.unsqueeze(0)).squeeze(0)\n",
    "            # Loss: matching the given κ plus a term to encourage low R\n",
    "            loss_val = F.mse_loss(k_pred, k_target) + beta * R_param\n",
    "            loss_val.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # Clamp the constraints (without breaking the gradient tracking in future steps,\n",
    "            # we do this in no_grad):\n",
    "            with torch.no_grad():\n",
    "                x_bar[0].clamp_(0.0, 1.0)\n",
    "                R_param.clamp_(1e-6, 1.0)\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f\"Restart {r+1}, Step {i+1}/{steps}: Loss={loss_val.item():.4f}, R={R_param.item():.4f}\")\n",
    "\n",
    "        if loss_val.item() < best_loss:\n",
    "            best_loss = loss_val.item()\n",
    "            best_x = x_mod.detach().clone()\n",
    "            best_k = k_pred.detach().clone()\n",
    "            best_R_val = R_param.detach().clone()\n",
    "    \n",
    "    return best_x, best_k, best_R_val, best_loss\n",
    "\n",
    "# Example: using an existing κ tensor (here bk) as k_target.\n",
    "xB_opt, kB_opt, R_opt, final_loss = design_least_phase_contrast_given_k(bk_vrnn,\n",
    "                                                                        restarts=3,\n",
    "                                                                        lr=1e-2,\n",
    "                                                                        steps=500,\n",
    "                                                                        beta=0.1)\n",
    "print(\"Optimized sample x (raw):\", xB_opt.cpu().numpy())\n",
    "print(\"Predicted κ:\", kB_opt.cpu().numpy())\n",
    "print(\"Optimized phase contrast R:\", R_opt.item())\n",
    "print(\"Final loss:\", final_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8057858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# C.  minimise κ̄ at fixed R  (thermal insulator)\n",
    "# TODO: make it work for both normalized and vanilla models\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def design_low_kbar_fixed_R(R,\n",
    "                            restarts=1,\n",
    "                            lr     = 1e-2,   \n",
    "                            steps  = 800):\n",
    "\n",
    "    best_val, best_xbar, best_k = -float('inf'), None, None\n",
    "    \n",
    "    for _ in range(restarts):\n",
    "        x, _ = random_raw_sample()  # random raw sample\n",
    "        x[-2:] = torch.tensor([1.0/R, R], device=device, dtype=dtype)  # enforce (1/R, R)\n",
    "        x_bar = encode_x(x)                                            # x_bar in [0,1]\n",
    "        x_bar.requires_grad_(True) \n",
    "        opt = torch.optim.Adam([x_bar], lr=lr)\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            opt.zero_grad()\n",
    "            k_raw, k_bar = raw_forward_from_xbar(x_bar)\n",
    "            loss  = kbar(k_raw) # - anisotropy(k_raw)\n",
    "            loss.backward()\n",
    "            x_bar.grad[-2:] = 0  # do not backprop through the last two features (1/R, R)\n",
    "            opt.step()  \n",
    "            \n",
    "            print(f\"Step {_+1}/{steps}: \"\n",
    "                  f\"kbar = {loss.item():.4f}\")\n",
    "            \n",
    "            # clamp *without* breaking the graph:\n",
    "            with torch.no_grad():\n",
    "                x_bar[0].clamp_(0.25, 0.75)    # keep volume fraction in [0.25, 0.75]\n",
    "        \n",
    "        val = loss.item()\n",
    "        if val > best_val:\n",
    "            best_val, best_xbar, best_k = val, x_bar.detach().clone(), k_raw.detach()\n",
    "            \n",
    "    best_x = decode_x(best_xbar) \n",
    "\n",
    "    return (best_x,                            # 53-D (raw)\n",
    "            best_k,                            # κ_raw\n",
    "            best_val)                          # anisotropy\n",
    "\n",
    "\n",
    "# C  – minimise mean κ at R = 5\n",
    "xC, kC, kbarC = design_low_kbar_fixed_R(R=1./100)\n",
    "print(\"k̄ =\", kbarC)\n",
    "print(\"reached κ =\", kC.detach().cpu().numpy())\n",
    "print(\"reached x =\", xC.detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vrnn_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
